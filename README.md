# AI Project Structure: digitaltwin

This document describes the folder structure and purpose of each component within the `ai_env` directory. The project is divided into two main parts:

- **Part 1**: Email tone fine-tuning
- **Part 2**: Document-based knowledge embedding and training

---

## Root Structure

```
/home/ubuntu/ai_env/
├── app.py                      # Main API app (if used)
├── fetch_and_train.py         # Web crawler + PDF downloader + trainer
├── check_gpt_available_models.py  # Model availability checker
├── auto_retrain.sh            # (Optional) retraining shell script
├── lib/                       # Internal Python libraries or dependencies
├── bin/                       # (Auto-generated by virtualenv)
├── include/                   # (Auto-generated by virtualenv)
├── part1/                     # Part 1: Email tone training
├── part2/                     # Part 2: Document processing and embedding
├── documents/                 # All documents used by the system
├── feedback_chunks/           # Optional: user feedback storage
├── feedback_faiss/            # Optional: feedback for vector search
├── faiss_index/               # FAISS vector index (if saved)
├── logs/                      # Training logs
│   └── auto_training_log.txt  # Auto-generated log file
```

---

## part1/: Email Tone Fine-Tuning

```
part1/
├── output/
│   ├── original_emails_fulltext/      # Raw emails (plain text)
│   └── fine_tune_emails_original.jsonl  # JSONL for fine-tuning
├── extract_and_redact_from_html.py   # HTML cleaner and extractor
├── part_1_generate_email_finetune_data.py  # Prompt + completion builder
```

## part2/: Document Knowledge Training

```
part2/
├── output/               # Converted .txt documents
├── auto_process.py       # Convert PDF/DOCX/TXT -> .txt, then chunk and train
```

---

## documents/: Project Document Storage

```
documents/
├── AI_Training_Material/      # Working input folder for part2 (PDF, DOCX)
├── txt/                       # Old .txt folder (deprecated, now moved)
├── processed/                 # Final processed .txt (optional future use)
├── s3_import/                 # AWS S3 import (raw mirror)
│   └── RCS/...                # S3 data with folders: Monitor, Product, Scope...
├── all_training_docs/         # Human-cleaned docs for final training
    └── RCS/...                # Same as above, better organized
```

---

## logs/

```
logs/
└── auto_training_log.txt      # Log entries for processed documents
```

---

## Notes

- All vector indexing (e.g., FAISS) and chunking is expected to happen in Part 2.
- You can continue expanding either part independently, and this structure ensures clarity and separation.

---

## GitHub Upload Tips

- Add a `.gitignore` to exclude `logs/`, `faiss_index/`, `__pycache__/`, and virtualenv folders like `bin/`, `include/`, `lib/`
- Main scripts to track: `auto_process.py`, `fetch_and_train.py`, `part_1_generate_email_finetune_data.py`

---

Last updated: 2025-07-04

