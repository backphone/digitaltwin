from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
import os

# æ–‡æ¡£ç›®å½•å’ŒåµŒå…¥æ¨¡å‹
DOC_DIR = "/home/ubuntu/ai_env/documents/AI_Training_Material/test"
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-MiniLM-L3-v2")

# è¯»å–æ‰€æœ‰txtæ–‡ä»¶å†…å®¹ï¼ˆé‡åˆ°ä¹±ç è‡ªåŠ¨è·³è¿‡ï¼‰
docs = []
for root, _, files in os.walk(DOC_DIR):
    for file in files:
        if file.endswith(".txt"):
            path = os.path.join(root, file)
            try:
                with open(path, encoding="utf-8", errors="ignore") as f:
                    content = f.read()
                    docs.append(content)
            except Exception as e:
                print(f"âš ï¸ è·³è¿‡æ–‡ä»¶ {path}: {e}")

print(f"ğŸ“„ åŠ è½½æ–‡æœ¬æ–‡ä»¶æ•°é‡: {len(docs)}")

# ç®€å•åˆ†å—å¤„ç†
chunks = []
chunk_size = 800
overlap = 100
for doc in docs:
    for start in range(0, len(doc), chunk_size - overlap):
        end = min(start + chunk_size, len(doc))
        chunk = doc[start:end]
        if chunk.strip():
            chunks.append(chunk)

print(f"ğŸ”¹ æ€»åˆ†å—æ•°: {len(chunks)}")

# æ„å»ºå¹¶ä¿å­˜å‘é‡ç´¢å¼•ï¼ˆLangChainæ ¼å¼ï¼‰
vectorstore = FAISS.from_texts(chunks, embeddings)
vectorstore.save_local("/home/ubuntu/ai_env/faiss_index")
print("âœ… ç”¨ LangChain æ–¹å¼é‡æ–°ä¿å­˜ç´¢å¼•ï¼")
