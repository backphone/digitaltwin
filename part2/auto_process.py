import os
import pickle
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from utils import load_file_content

# æ¯æ¬¡é‡æ–°åˆå§‹åŒ– FAISS ç´¢å¼•å’Œ metadata åˆ—è¡¨
index = faiss.IndexFlatL2(384)  # æˆ–ä½ å®é™…çš„ç»´åº¦
metadata = []

# è·¯å¾„é…ç½®
#DOC_DIR = "/home/ubuntu/ai_env/documents/AI_Training_Material"
DOC_DIR = "/home/ubuntu/ai_env/documents/AI_Training_Material/test"
INDEX_PATH = "/home/ubuntu/ai_env/faiss_index/index.faiss"
META_PATH = "/home/ubuntu/ai_env/faiss_index/index.pkl"

# æ¨¡å‹å’Œç´¢å¼•åˆå§‹åŒ–
EMBED_MODEL = "all-MiniLM-L6-v2"
embedder = SentenceTransformer(EMBED_MODEL)

# åŠ è½½æˆ–æ–°å»ºç´¢å¼•å’Œå…ƒæ•°æ®
def load_existing_index(index_path, meta_path):
    if os.path.exists(index_path) and os.path.exists(meta_path):
        print("ğŸŸ¡ Loading existing FAISS index and metadata...")
        index = faiss.read_index(index_path)
        with open(meta_path, "rb") as f:
            metadata = pickle.load(f)
            if isinstance(metadata, tuple):
                metadata = list(metadata)
        return index, metadata
    else:
        print("ğŸŸ¢ Creating new FAISS index...")
        index = faiss.IndexFlatL2(384)
        metadata = []
        return index, metadata

index, metadata = load_existing_index(INDEX_PATH, META_PATH)

# æ–‡æœ¬åˆ‡åˆ†,smaller chunk size means higher granuity,
def split_text(text, chunk_size=800, overlap=100):
    chunks = []
    start = 0
    while start < len(text):
        end = min(start + chunk_size, len(text))
        chunks.append(text[start:end])
        start += chunk_size - overlap
    return chunks

# æ–‡ä»¶å¤„ç†é€»è¾‘
def process_file(filepath):
    global metadata
    print(f"ğŸ” Processing file: {filepath}")
    try:
        content = load_file_content(filepath)
        if not content.strip():
            print(f"âš ï¸ Skipped (empty file)")
            return

        chunks = split_text(content)
        embeddings = embedder.encode(chunks)

        for i, vec in enumerate(embeddings):
            index.add(np.array([vec]))
            metadata.append({
                "source": f"{filepath} :: chunk {i+1}",
                "text": chunks[i]  # âœ… æ·»åŠ è¿™ä¸€è¡Œ
        })

    except Exception as e:
        print(f"âŒ Error processing {filepath}: {e}")

# éå†æ–‡æ¡£ç›®å½•
for root, _, files in os.walk(DOC_DIR):
    for file in files:
        filepath = os.path.join(root, file)
        process_file(filepath)

# ä¿å­˜ç´¢å¼•å’Œå…ƒæ•°æ®
faiss.write_index(index, INDEX_PATH)
with open(META_PATH, "wb") as f:
    pickle.dump(metadata, f)

print("âœ… Processing completed and index saved.")

print(f"ğŸ§  Total chunks saved: {len(metadata)}")

